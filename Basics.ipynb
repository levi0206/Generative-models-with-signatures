{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Basics\n",
    "As an opening, we first mention some important concepts in deep learning, including\n",
    "- Artificial Neural Network\n",
    "- Automatic Differentiation\n",
    "- Monte Carlo Estimation \n",
    "- Stochastic Gradient Descent.\n",
    "\n",
    "Given the extensive nature of each subsection, a comprehensive coverage is beyond the scope. Instead, we will pick certain subjects, explain some important ideas and theorems that suuport these mechanisms, and provide a simple example as demonstration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network, or abreviated neural network in machine learning context, is the core of deep learning. The name and structure are inspired by human brain, mimicking the interactions of biological neurons. Neural networks consists of an input layer, one or multiple hidden layers, and an output layer. As a toy example, a neural network with one hidden layer is like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute\n",
    "nn.Linear(input_dim,output_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, one can implement the neural network with different tricks: adjusting the depth (number of layers) of neural network or width (number of neurons) of a layer, using dropout [number], using batch normalization [number], or even choosing another activation function if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universal approximation theorem is a key factor contributing to the widespread application of neural networks across various fields and scenarios. The universal approximation theorem originally is given by Cybenko in 1989 [number] using sigmoid functions.\n",
    "\n",
    "**Theorem (Universal Approximation Theorem).** Let $\\sigma$ be any continuous sigmoidal function. Then finite sums of the form\n",
    "$$\n",
    "G(x)=\\sum_{j=1}^N \\alpha_j\\sigma(y_j^T x+\\theta_j)\n",
    "$$\n",
    "are dense in the space of continuous functions $C([0,1]^n)=C(I_n)$. In other words, given any $f\\in C(I_n)$,there is a sum, $G(x)$, of the above form, for which\n",
    "$$\n",
    "|G(x)-f(x)|<\\epsilon \\quad \\text{for all } x\\in I_n.\n",
    "$$\n",
    "This original universal approximation theorem says the neural network with one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute\n",
    "neural_network = nn.Sequential(\n",
    "    nn.Linear(N,1),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can approximate any continuous function on $[0,1]^n$, where $N$ is the number of neurons. Now there are many variants and generalizations of the theorem. For example, the universality can be interpreted by neural ordinary differential equations, as referenced in [number].\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p(\\mathbf{x},\\boldsymbol{\\theta})$ be some probability distribution depending on a collection $\\boldsymbol{\\theta}$ of parameters. Consider the expectation of the form \n",
    "$$\n",
    "\\mathcal{F}(\\boldsymbol{\\theta})=\\int p(\\mathbf{x},\\boldsymbol{\\theta})f(\\mathbf{x},\\boldsymbol{\\phi})d\\mathbf{x}=\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right]\n",
    "$$\n",
    "where $\\mathbf{x}$ is the input of objective $f$ with probability $p(\\mathbf{x},\\boldsymbol{\\theta})$, and $\\boldsymbol{\\phi}$ is a set of the parameters of $f$. Of course, $\\boldsymbol{\\phi}$ might be equal to $\\boldsymbol{\\theta}$. We are interested in learning the parameters $\\boldsymbol{\\theta}$, which requires the computation of the gradient of $\\mathcal{F}(\\boldsymbol{\\theta})$ with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\mathcal{F}(\\boldsymbol{\\theta})=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right].\n",
    "$$\n",
    "The expectation in general is intractable because the distribution $p(\\mathbf{x},\\boldsymbol{\\theta})$ might be high-dimensional, in deep learning, easily in the order of thousands or even more of dimensions, and very complicated. Moreover, the function $f$ might be non-differentiable, or a black-box function which the output is all we observe, artificial neural network for example. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo Method provides another insight of this sort of impossible calculation. Instead of computing the closed form of the integral directly, we draw i.i.d. samples $\\hat{\\mathbf{x}}^{(1)},...,\\hat{\\mathbf{x}}^{(S)}$ from $p(\\mathbf{x},\\boldsymbol{\\theta})$, and approximate the integral with the average of $f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})$, called a Monte Carlo estimator:\n",
    "$$\n",
    "\\bar{\\mathcal{F}}_S=\\frac{1}{S}\\sum_{i=1}^S f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi}).\n",
    "$$\n",
    "Although $\\bar{\\mathcal{F}}_S$ is still a random variable because it depends on random variables $\\hat{\\mathbf{x}}^{(1)},...,\\hat{\\mathbf{x}}^{(S)}$, now it is equiped with desirable properties:\n",
    "\n",
    "**Unbiasedness.**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[\\bar{\\mathcal{F}}_S\\right] & = \\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[\\frac{1}{S}\\sum_{i=1}^S f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})\\right] = \\frac{1}{S}\\sum_{i=1}^S\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[ f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})\\right] \\\\\n",
    "& = \\frac{1}{S}\\sum_{i=1}^S\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right] = \\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "Unbiasedness is always preferred because it allows us to guarantee the convergence of a stochastic optimisation procedure.\n",
    "\n",
    "**Consistency.** \n",
    "By strong law of large numbers, the random variable $\\bar{\\mathcal{F}}_S$ converges to $\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right]$ almost surely as the number of samples $S$ increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo estimation provides a convenient approach to approximate expectations. For example, in generative adversarial networks, the discriminator is updated with the loss function\n",
    "$$\n",
    "-\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim p_{data}}\\left[\\log D(\\hat{\\mathbf{x}})\\right]-\\mathbb{E}_{\\hat{\\mathbf{z}}\\sim \\mathcal{N}(0,\\mathbf{I})} \\left[\\log (1-D(G(\\hat{\\mathbf{z}})))\\right].\n",
    "$$\n",
    "where $D$ is the discriminator network, $G$ is the generator network, $x$ is a datapoint sampled from data distribution $p_{data} $ and $z$ is a random noise vector sampled from $\\mathcal{N}(0,\\mathbf{I})$. With Monte Carlo estimation, instead of calculating two intractable integrals, we only need to calculate the average\n",
    "$$\n",
    "-\\frac{1}{m}\\sum_{i=1}^m \\ln D(\\hat{\\mathbf{x}}^{(i)})-\\frac{1}{m}\\sum_{i=1}^m \\ln(1-D(G(\\hat{\\mathbf{z}}^{(i)}))).\n",
    "$$\n",
    "The quantity $m$, or batch size, is a hyperparameter. Even though larger $m$ gives better approximation of the expectation, too large $m$ may induce inferior performance in the optimization procedure [number]. The only way to find \"better\" $m$ is through numerous trials with patience. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- N. Srivastava et al. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. In *Journal of Machine Learning Research*, 2014.\n",
    "- S. Ioffe et al. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In *Journal of Machine Learning Research*, 2015.\n",
    "- D. Masters et al. Revisiting Small Batch Training for Deep Neural Networks, *arXiv:1804.07612*, 2018.\n",
    "- G. Cybenko. Approximation by superpositions of a sigmoidal function. In *Springer*, 1989.\n",
    "- P. Kidger. On Neural Differential Equations, Oxford University, 2022.\n",
    "- S. Mohamed et al. Figurnov and A. Mnih. Monte Carlo Gradient Estimation in Machine Learning. In *Journal of Machine Learning Research*, 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
