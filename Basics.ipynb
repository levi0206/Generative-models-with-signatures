{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Basics\n",
    "As an opening, we first mention some important concepts in deep learning, including\n",
    "- Artificial Neural Network\n",
    "- Automatic Differentiation and Backpropogation\n",
    "- Monte Carlo Estimation \n",
    "- Stochastic Gradient Descent.\n",
    "\n",
    "Given the extensive nature of each subsection, a comprehensive coverage is beyond the scope. Instead, we will pick certain subjects, explain some important ideas and theorems that suuport these mechanisms, and provide a simple example as demonstration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network, or abreviated neural network in machine learning context, is the core of deep learning. The name and structure are inspired by human brain, mimicking the interactions of biological neurons. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network consists of an input layer, one or multiple hidden layers, and an output layer. As a toy example, a neural network is like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute\n",
    "nn.Linear(input_dim,output_dim) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which performs the linear transformations on $X_{n\\times i}\\in\\mathbb{R}^{n\\times i}$:\n",
    "$$\n",
    "Y_{n\\times o} = X_{n\\times i}W_{i\\times o}+b_{o}\n",
    "$$\n",
    "where $W_{i\\times o}$ is a weight matrix and $b_o$ is a bias vector. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can implement a neural network with three layers as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute\n",
    "nn.Sequential(\n",
    "    nn.Linear(input_dim,hidden_1),\n",
    "    nn.Linear(hidden_1,hidden_2),\n",
    "    nn.Linear(hidden_2,output_dim)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, one can implement the neural network with different tricks: adjusting the depth (number of layers) of neural network or width (number of neurons) of a layer, using dropout [number], using batch normalization [number], or even choosing another activation function if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universal approximation theorem is a key factor contributing to the widespread application of neural networks across various fields and scenarios. The universal approximation theorem originally is given by Cybenko in 1989 [number] using sigmoidal functions\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma(x)=\n",
    "\\begin{cases}\n",
    "1, & x\\to \\infty \\\\\n",
    "0, & x\\to-\\infty\n",
    "\\end{cases}.\n",
    "\\end{equation}\n",
    "$$\n",
    "Denote $I_n=[0,1]^n$ the $n$-dimensional unit cube and $C(I_n,\\mathbb{R})$ space of continuous functions from $I_n$ to $\\mathbb{R}$.\n",
    "\n",
    "**Theorem (Universal Approximation Theorem, G.Cybenko).** Let $\\sigma$ be any continuous sigmoidal function, $f\\in C(I_n,\\mathbb{R})$. Then finite sums of the form\n",
    "$$\n",
    "G^N(x)=\\sum_{j=1}^N \\alpha_j\\sigma(w_j^T x+\\theta_j)\n",
    "$$\n",
    "are dense in the space of continuous functions $C(I_n,\\mathbb{R})$. In other words, given any $f\\in C(I_n,\\mathbb{R})$, there exists $N\\in\\mathbb{N}$ such that\n",
    "$$\n",
    "|G^N(x)-f(x)|<\\epsilon \\quad \\text{for all } x\\in I_n.\n",
    "$$\n",
    "This original universal approximation theorem says a neural network with one layer and arbitrary width can approximate any continuous function on $[0,1]^n$, where $N$ is the number of neurons. Now there are many variants and generalizations of the theorem, for example [number],[number]. Another insight of the universality of neural network can be interpreted by ordinary differential equations, as referenced in [number]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation and Backpropogation\n",
    "Automatic differentiation is widely used for deep learning optimization. In a nutshell, automatic differentiation is a clever way of performing chain rule without manually computing derivatives manually. \n",
    "\n",
    "Let $f=f_u\\circ f_{u-1}\\circ \\cdots \\circ f_1$ be our neural network. Each $f_i$ is differentiable. Chain rule of differentiation tells us how to compute the derivative:\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{df_u}{df_{u-1}}\\cdots\\frac{df_{2}}{df_{1}}\\frac{df_1}{dx}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p(\\mathbf{x},\\boldsymbol{\\theta})$ be some probability distribution depending on a collection $\\boldsymbol{\\theta}$ of parameters. Consider the expectation of the form \n",
    "$$\n",
    "\\mathcal{F}(\\boldsymbol{\\theta})=\\int p(\\mathbf{x},\\boldsymbol{\\theta})f(\\mathbf{x},\\boldsymbol{\\phi})d\\mathbf{x}=\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right]\n",
    "$$\n",
    "where $\\mathbf{x}$ is the input of objective $f$ with probability $p(\\mathbf{x},\\boldsymbol{\\theta})$, and $\\boldsymbol{\\phi}$ is a set of the parameters of $f$. Of course, $\\boldsymbol{\\phi}$ might be equal to $\\boldsymbol{\\theta}$. We are interested in learning the parameters $\\boldsymbol{\\theta}$, which requires the computation of the gradient of $\\mathcal{F}(\\boldsymbol{\\theta})$ with respect to $\\theta$:\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\mathcal{F}(\\boldsymbol{\\theta})=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right].\n",
    "$$\n",
    "The expectation in general is intractable because the distribution $p(\\mathbf{x},\\boldsymbol{\\theta})$ might be high-dimensional, in deep learning, easily in the order of thousands or even more of dimensions, and very complicated. Moreover, the function $f$ might be non-differentiable, or a black-box function which the output is all we observe, artificial neural network for example. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo Method provides another insight of this sort of impossible calculation. Instead of computing the closed form of the integral directly, we draw i.i.d. samples $\\hat{\\mathbf{x}}^{(1)},...,\\hat{\\mathbf{x}}^{(S)}$ from $p(\\mathbf{x},\\boldsymbol{\\theta})$, and approximate the integral with the average of $f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})$, called a Monte Carlo estimator:\n",
    "$$\n",
    "\\bar{\\mathcal{F}}_S=\\frac{1}{S}\\sum_{i=1}^S f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi}).\n",
    "$$\n",
    "Although $\\bar{\\mathcal{F}}_S$ is still a random variable because it depends on random variables $\\hat{\\mathbf{x}}^{(1)},...,\\hat{\\mathbf{x}}^{(S)}$, now it is equiped with desirable properties:\n",
    "\n",
    "**Unbiasedness.**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[\\bar{\\mathcal{F}}_S\\right] & = \\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[\\frac{1}{S}\\sum_{i=1}^S f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})\\right] = \\frac{1}{S}\\sum_{i=1}^S\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[ f(\\hat{\\mathbf{x}}^{(i)},\\boldsymbol{\\phi})\\right] \\\\\n",
    "& = \\frac{1}{S}\\sum_{i=1}^S\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right] = \\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "Unbiasedness is always preferred because it allows us to guarantee the convergence of a stochastic optimisation procedure.\n",
    "\n",
    "**Consistency.** \n",
    "By strong law of large numbers, the random variable $\\bar{\\mathcal{F}}_S$ converges to $\\mathbb{E}_{\\mathbf{x}\\sim p(\\mathbf{x},\\boldsymbol{\\theta})}\\left[f(\\mathbf{x},\\boldsymbol{\\phi})\\right]$ almost surely as the number of samples $S$ increases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo estimation provides a convenient approach to approximate expectations. For example, in generative adversarial networks, the discriminator is updated with the loss function\n",
    "$$\n",
    "-\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim p_{data}}\\left[\\log D(\\hat{\\mathbf{x}})\\right]-\\mathbb{E}_{\\hat{\\mathbf{z}}\\sim \\mathcal{N}(0,\\mathbf{I})} \\left[\\log (1-D(G(\\hat{\\mathbf{z}})))\\right].\n",
    "$$\n",
    "where $D$ is the discriminator network, $G$ is the generator network, $x$ is a datapoint sampled from data distribution $p_{data} $ and $z$ is a random noise vector sampled from $\\mathcal{N}(0,\\mathbf{I})$. With Monte Carlo estimation, instead of calculating two intractable integrals, we only need to calculate the average\n",
    "$$\n",
    "-\\frac{1}{m}\\sum_{i=1}^m \\ln D(\\hat{\\mathbf{x}}^{(i)})-\\frac{1}{m}\\sum_{i=1}^m \\ln(1-D(G(\\hat{\\mathbf{z}}^{(i)}))).\n",
    "$$\n",
    "The quantity $m$, or batch size, is a hyperparameter. Even though larger $m$ gives better approximation of the expectation, too large $m$ may induce inferior performance in the optimization procedure [number]. The only way to find \"better\" $m$ is through numerous trials with patience. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Minibatch) Stochastic Gradient Descent\n",
    "Nowadays, people usually prefer minibatch stochastic gradient descent to optimize neural networks, as it is more computationally efficient than gradient descent or stochastic gradient descent. \n",
    "\n",
    "Let $\\ell$ be our loss function. Let $n$ be the number of training data point, $b$ the batch size, $\\boldsymbol{\\theta^0}\\in\\mathbb{R}^d$ the initial parameter of our neural network and $(\\alpha_t)_{t\\in\\mathbb{N}}$ a sequence of step size, or learning rate. Given a subsest $B\\subset \\{1,...,n\\}$, we define\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta^t}} \\ell_B(\\boldsymbol{\\theta^t})=\\frac{1}{|B|}\\sum_{i\\in B} \\nabla_{\\boldsymbol{\\theta^t}} \\ell_i(\\boldsymbol{\\theta^t})\n",
    "$$\n",
    "The minibatch stochastic gradient descent algorithm is given by \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& B_t\\subset\\{1,...,n\\} \\quad\\quad \\text{Sampled uniformly among sets of size $b$} \\\\\n",
    "& \\boldsymbol{\\theta^{t+1}} = \\boldsymbol{\\theta^t}-\\alpha_t \\nabla_{\\boldsymbol{\\theta^t}} \\ell_B(\\boldsymbol{\\theta^t}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that $\\nabla_{\\boldsymbol{\\theta^t}} \\ell_B(\\boldsymbol{\\theta^t})$ is an unbiased estimator\n",
    "$$\n",
    "\\mathbb{E}_{b}\\left[\\nabla_{\\boldsymbol{\\theta^t}} \\ell_B(\\boldsymbol{\\theta^t})\\right] = \\frac{1}{{n \\choose b}} \\sum_{\\substack{B\\subset\\{1,...,n\\} \\\\ |B|=b}} \\nabla_{\\boldsymbol{\\theta^t}} \\ell_B(\\boldsymbol{\\theta^t}) = \\nabla_{\\boldsymbol{\\theta^t}} \\ell(\\boldsymbol{\\theta^t})\n",
    "$$\n",
    "because each batch is sampled with probability $\\frac{1}{{n \\choose b}}$. For details on the convergence theorem applicable to convex and smooth functions, please refer to [number].\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- N. Srivastava et al. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. In *Journal of Machine Learning Research*, 2014.\n",
    "- S. Ioffe et al. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In *Journal of Machine Learning Research*, 2015.\n",
    "- D. Masters et al. Revisiting Small Batch Training for Deep Neural Networks, *arXiv:1804.07612*, 2018.\n",
    "- G. Cybenko. Approximation by superpositions of a sigmoidal function. In *Springer*, 1989.\n",
    "- A. Pinkus. Approximation theory of the MLP model in neural networks, 1999.\n",
    "- DX Zhou. Universality of deep convolutional neural networks, 2020.\n",
    "- P. Kidger. On Neural Differential Equations, Oxford University, 2022.\n",
    "- S. Mohamed et al. Monte Carlo Gradient Estimation in Machine Learning. In *Journal of Machine Learning Research*, 2020.\n",
    "- S. Ruder. An overview of gradient descent optimization algorithms, 2016.\n",
    "- S. Bubeck. Convex Optimization: Algorithms and Complexity, *Foundations and Trends® in Machine Learning*, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
